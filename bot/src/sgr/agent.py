"""
SGR Agent - –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π Deep Research –∞–≥–µ–Ω—Ç.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç sgr-agent-core –Ω–∞–ø—Ä—è–º—É—é –±–µ–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞.
–° –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π Langfuse –¥–ª—è observability.
"""
import asyncio
import json
import logging
from dataclasses import dataclass, field
from typing import AsyncIterator, Type, Optional

from openai import AsyncOpenAI
from sgr_agent_core import (
    AgentConfig,
    LLMConfig,
    SearchConfig,
    ExecutionConfig,
    PromptsConfig,
    AgentStatesEnum,
    BaseTool,
)
from sgr_agent_core.agents import SGRAgent
from sgr_agent_core.tools import (
    WebSearchTool,
    ExtractPageContentTool,
    GeneratePlanTool,
    ReasoningTool,
    ClarificationTool,
    AdaptPlanTool,
    CreateReportTool,
    FinalAnswerTool,
    NextStepToolsBuilder,
)

logger = logging.getLogger(__name__)

# Registry: tool name -> tool class
TOOL_REGISTRY: dict[str, Type[BaseTool]] = {
    "WebSearchTool": WebSearchTool,
    "ExtractPageContentTool": ExtractPageContentTool,
    "GeneratePlanTool": GeneratePlanTool,
    "ReasoningTool": ReasoningTool,
    "ClarificationTool": ClarificationTool,
    "AdaptPlanTool": AdaptPlanTool,
    "CreateReportTool": CreateReportTool,
    "FinalAnswerTool": FinalAnswerTool,
}

# Base system prompt from sgr-agent-core + validation instructions
SYSTEM_PROMPT_WITH_VALIDATION = """<MAIN_TASK_GUIDELINES>
You are an expert researcher with adaptive planning and schema-guided-reasoning capabilities. You get the research task and you neeed to do research and genrete answer
</MAIN_TASK_GUIDELINES>

<DATE_GUIDELINES>
PAY ATTENTION TO THE DATE INSIDE THE USER REQUEST
DATE FORMAT: YYYY-MM-DD HH:MM:SS (ISO 8601)
IMPORTANT: The date above is in YYYY-MM-DD format (Year-Month-Day). For example, 2025-10-03 means October 3rd, 2025, NOT March 10th.
</DATE_GUIDELINES>

<IMPORTANT_LANGUAGE_GUIDELINES>: Detect the language from user request and use this LANGUAGE for all responses, searches, and result finalanswertool
LANGUAGE ADAPTATION: Always respond and create reports in the SAME LANGUAGE as the user's request.
If user writes in Russian - respond in Russian, if in English - respond in English.
</IMPORTANT_LANGUAGE_GUIDELINES>:

<CORE_PRINCIPLES>:
1. Memorize plan you generated in first step and follow the task inside your plan.
1. Adapt plan when new data contradicts initial assumptions
2. Search queries in SAME LANGUAGE as user request
3. Final Answer ENTIRELY in SAME LANGUAGE as user request
</CORE_PRINCIPLES>

<REASONING_GUIDELINES>
ADAPTIVITY: Actively change plan when discovering new data.
ANALYSIS EXTRACT DATA: Always analyze data that you took in extractpagecontenttool
</REASONING_GUIDELINES>

<PRECISION_GUIDELINES>
CRITICAL FOR FACTUAL ACCURACY:
When answering questions about specific dates, numbers, versions, or names:
1. EXACT VALUES: Extract the EXACT value from sources (day, month, year for dates; precise numbers for quantities)
2. VERIFY YEAR: If question mentions a specific year (e.g., "in 2022"), verify extracted content is about that SAME year
3. CROSS-VERIFICATION: When sources provide contradictory information, prefer:
   - Official sources and primary documentation over secondary sources
   - Search result snippets that DIRECTLY answer the question over extracted page content
   - Multiple independent sources confirming the same fact
4. DATE PRECISION: Pay special attention to exact dates - day matters (October 21 ‚â† October 22)
5. NUMBER PRECISION: For numbers/versions, exact match required (6.88b ‚â† 6.88c, Episode 31 ‚â† Episode 32)
6. SNIPPET PRIORITY: If search snippet clearly states the answer, trust it unless extract proves it wrong
7. TEMPORAL VALIDATION: When extracting page content, check if the page shows data for correct time period
</PRECISION_GUIDELINES>

<DATA_VALIDATION_GUIDELINES>
MANDATORY: Before using CreateReportTool, you MUST use ReasoningTool to validate collected data.

In ReasoningTool validation step, check:

1. SANITY CHECK - Are numbers realistic?
   - Compare with your general knowledge about typical ranges
   - If a number seems unusually high/low, flag it or search for verification

2. DATA CONFLICTS - Do sources agree?
   - List key numbers found with their sources
   - If numbers differ significantly, note the discrepancy in report

3. CONCLUSION ALIGNMENT - Does your conclusion match the data?
   - State which option is best BASED ON THE DATA you collected
   - If data shows X is best but you recommend Y, you MUST explain WHY

4. SOURCE RELIABILITY - Are sources trustworthy?
   - Prefer official statistics, established platforms (Glassdoor, LinkedIn, govt data)
   - Flag data from unknown or potentially unreliable sources

ONLY after ReasoningTool validation, proceed to CreateReportTool.
</DATA_VALIDATION_GUIDELINES>

<AGENT_TOOL_USAGE_GUIDELINES>:
{available_tools}
</AGENT_TOOL_USAGE_GUIDELINES>

<CRITICAL_LANGUAGE_REQUIREMENT>
IMPORTANT: Your ENTIRE response (CreateReportTool, FinalAnswerTool) MUST be in the SAME LANGUAGE as the user's question.
- User writes in Russian ‚Üí ALL your output in Russian
- User writes in English ‚Üí ALL your output in English
- This applies to: report content, conclusions, recommendations, all text
DO NOT mix languages. DO NOT respond in English if user asked in Russian.
</CRITICAL_LANGUAGE_REQUIREMENT>"""


def resolve_tools(tool_names: list[str]) -> list[Type[BaseTool]]:
    """Resolve tool names to tool classes."""
    tools = []
    for name in tool_names:
        if name in TOOL_REGISTRY:
            tools.append(TOOL_REGISTRY[name])
        else:
            logger.warning(f"Unknown tool: {name}, skipping")
    return tools


def parse_sse_tool_call(sse_data: str) -> tuple[str, str, dict] | None:
    """
    Parse SSE event to extract tool call info.

    Returns (tool_call_id, tool_name, arguments) or None if not a tool call.
    """
    if not sse_data.startswith("data: "):
        return None

    json_str = sse_data[6:].strip()  # Remove "data: " prefix
    if json_str == "[DONE]":
        return None

    try:
        data = json.loads(json_str)
        choices = data.get("choices", [])
        if not choices:
            return None

        delta = choices[0].get("delta", {})
        tool_calls = delta.get("tool_calls")

        if tool_calls and len(tool_calls) > 0:
            tc = tool_calls[0]
            tool_id = tc.get("id", "")
            func = tc.get("function", {})
            tool_name = func.get("name", "")
            arguments_str = func.get("arguments", "{}")

            # Parse arguments JSON
            try:
                arguments = json.loads(arguments_str) if arguments_str else {}
            except json.JSONDecodeError:
                arguments = {}

            if tool_name:
                return (tool_id, tool_name, arguments)

    except json.JSONDecodeError:
        pass

    return None

# Try to import langfuse and openinference
try:
    from langfuse import observe, Langfuse
    LANGFUSE_AVAILABLE = True
    langfuse_client = None  # Will be initialized if enabled
except ImportError:
    LANGFUSE_AVAILABLE = False
    observe = lambda *args, **kwargs: lambda f: f  # no-op decorator
    langfuse_client = None
    Langfuse = None

# Try to import OpenAI instrumentor for detailed LLM tracing
try:
    from openinference.instrumentation.openai import OpenAIInstrumentor
    OPENAI_INSTRUMENTOR_AVAILABLE = True
except ImportError:
    OPENAI_INSTRUMENTOR_AVAILABLE = False
    OpenAIInstrumentor = None


def setup_langfuse(
    public_key: str,
    secret_key: str,
    host: str = "https://cloud.langfuse.com",
) -> bool:
    """
    Setup Langfuse environment variables for tracing.
    Returns True if setup successful.
    """
    global langfuse_client
    try:
        import os
        os.environ["LANGFUSE_PUBLIC_KEY"] = public_key
        os.environ["LANGFUSE_SECRET_KEY"] = secret_key
        os.environ["LANGFUSE_HOST"] = host

        if LANGFUSE_AVAILABLE and Langfuse:
            # Initialize client to ensure connection works
            langfuse_client = Langfuse(
                public_key=public_key,
                secret_key=secret_key,
                host=host,
            )
            logger.info(f"Langfuse configured (host: {host})")

            # Enable OpenAI instrumentation for detailed LLM tracing
            if OPENAI_INSTRUMENTOR_AVAILABLE and OpenAIInstrumentor:
                OpenAIInstrumentor().instrument()
                logger.info("OpenAI instrumentation enabled (OpenInference)")

            return True
        else:
            logger.warning("Langfuse not installed")
            return False
    except Exception as e:
        logger.warning(f"Failed to setup Langfuse: {e}")
        return False


def create_openai_client(api_key: str, base_url: str) -> AsyncOpenAI:
    """Create standard OpenAI client."""
    return AsyncOpenAI(api_key=api_key, base_url=base_url)


@dataclass
class ResearchProgress:
    """–ü—Ä–æ–≥—Ä–µ—Å—Å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é."""

    step: int = 0
    tool_name: str = ""
    tool_emoji: str = ""
    description: str = ""
    searches_done: int = 0
    is_final: bool = False


# –ú–∞–ø–ø–∏–Ω–≥ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —ç–º–æ–¥–∑–∏ –∏ –æ–ø–∏—Å–∞–Ω–∏—è
TOOL_INFO = {
    "generateplantool": ("üìã", "–ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"),
    "websearchtool": ("üîç", "–ü–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ"),
    "extractpagecontenttool": ("üìÑ", "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"),
    "reasoningtool": ("üß†", "–ê–Ω–∞–ª–∏–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"),
    "clarificationtool": ("‚ùì", "–ó–∞–ø—Ä–æ—Å —É—Ç–æ—á–Ω–µ–Ω–∏—è"),
    "adaptplantool": ("üîÑ", "–ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –ø–ª–∞–Ω–∞"),
    "createreporttool": ("üìù", "–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á—ë—Ç–∞"),
    "finalanswertool": ("‚úÖ", "–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞"),
}


def get_tool_info(tool_name: str) -> tuple[str, str]:
    """–ü–æ–ª—É—á–∏—Ç—å —ç–º–æ–¥–∑–∏ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞."""
    key = tool_name.lower().replace("_", "")
    return TOOL_INFO.get(key, ("üîß", tool_name))


@dataclass
class ResearchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è."""

    content: str = ""
    is_done: bool = False
    needs_clarification: bool = False
    clarification_question: str | None = None
    tools_used: list[str] = field(default_factory=list)
    iterations: int = 0
    error: str | None = None
    progress: ResearchProgress | None = None  # –î–æ–±–∞–≤–ª–µ–Ω–æ –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞


class TelegramResearchAgent(SGRAgent):
    """
    –ö–∞—Å—Ç–æ–º–Ω—ã–π SGR –∞–≥–µ–Ω—Ç —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º tools.

    –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç _prepare_tools() –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
    –ø—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ –ª–∏–º–∏—Ç–æ–≤ –∏—Ç–µ—Ä–∞—Ü–∏–π, clarifications –∏ –ø–æ–∏—Å–∫–æ–≤.

    Tools –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ config, –∞ –Ω–µ —Ö–∞—Ä–¥–∫–æ–¥—è—Ç—Å—è.
    """

    def __init__(
        self,
        task_messages: list[dict],
        openai_client: AsyncOpenAI,
        agent_config: AgentConfig,
        toolkit: list[Type[BaseTool]],
        **kwargs,
    ):
        super().__init__(
            task_messages=task_messages,
            openai_client=openai_client,
            agent_config=agent_config,
            toolkit=toolkit,
            **kwargs,
        )

    async def _prepare_tools(self):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ tools –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è.

        –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —É–±–∏—Ä–∞–µ—Ç tools –ø—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ –ª–∏–º–∏—Ç–æ–≤:
        - max_iterations ‚Üí —Ç–æ–ª—å–∫–æ CreateReportTool, FinalAnswerTool
        - max_clarifications ‚Üí —É–±—Ä–∞—Ç—å ClarificationTool
        - max_searches ‚Üí —É–±—Ä–∞—Ç—å WebSearchTool
        """
        tools = set(self.toolkit)

        # –ü—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ –ª–∏–º–∏—Ç–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π - —Ç–æ–ª—å–∫–æ –∑–∞–≤–µ—Ä—à–∞—é—â–∏–µ tools
        if self._context.iteration >= self.config.execution.max_iterations:
            logger.info(f"Max iterations reached ({self._context.iteration}), limiting to final tools")
            tools = {CreateReportTool, FinalAnswerTool}

        # –ò—Å—á–µ—Ä–ø–∞–ª–∏ clarifications - —É–±—Ä–∞—Ç—å ClarificationTool
        if self._context.clarifications_used >= self.config.execution.max_clarifications:
            logger.debug(f"Max clarifications reached ({self._context.clarifications_used})")
            tools -= {ClarificationTool}

        # –ò—Å—á–µ—Ä–ø–∞–ª–∏ –ø–æ–∏—Å–∫–∏ - —É–±—Ä–∞—Ç—å WebSearchTool –∏ GeneratePlanTool
        # (GeneratePlanTool —É–±–∏—Ä–∞–µ–º —á—Ç–æ–±—ã –∞–≥–µ–Ω—Ç –Ω–µ –∑–∞—Å—Ç—Ä—è–ª –≤ —Ü–∏–∫–ª–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è)
        if self._context.searches_used >= self.config.search.max_searches:
            logger.info(f"Max searches reached ({self._context.searches_used}), limiting to synthesis tools")
            tools -= {WebSearchTool, GeneratePlanTool}

        return NextStepToolsBuilder.build_NextStepTools(list(tools))


class DeepResearchAgent:
    """
    Deep Research –∞–≥–µ–Ω—Ç –Ω–∞ –±–∞–∑–µ SGR Agent Core.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Schema-Guided Reasoning –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
    —Å –ø–æ–∏—Å–∫–æ–º –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ —á–µ—Ä–µ–∑ Tavily.

    –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è tools –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –∏–∑ config.yaml.
    –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –±–µ—Ä—ë—Ç—Å—è –∏–∑ sgr-agent-core (–ø–æ–ª–Ω—ã–π, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π).
    –Ø–∑—ã–∫ –æ—Ç–≤–µ—Ç–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ —è–∑—ã–∫—É –∑–∞–ø—Ä–æ—Å–∞.
    """

    def __init__(
        self,
        anthropic_api_key: str,
        tavily_api_key: str,
        model: str = "claude-haiku-4-5",
        api_base: str = "https://api.anthropic.com/v1",
        temperature: float = 0.4,
        max_tokens: int = 8000,
        max_iterations: int = 10,
        max_searches: int = 4,
        max_clarifications: int = 3,
        # Tools from config
        toolkit: list[Type[BaseTool]] | None = None,
        # Langfuse settings
        langfuse_enabled: bool = False,
        langfuse_public_key: str = "",
        langfuse_secret_key: str = "",
        langfuse_host: str = "https://cloud.langfuse.com",
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞.

        Args:
            anthropic_api_key: API –∫–ª—é—á LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
            tavily_api_key: API –∫–ª—é—á Tavily –¥–ª—è –≤–µ–±-–ø–æ–∏—Å–∫–∞
            model: –ú–æ–¥–µ–ª—å LLM
            api_base: Base URL API
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            max_tokens: –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤
            max_iterations: –ú–∞–∫—Å–∏–º—É–º –∏—Ç–µ—Ä–∞—Ü–∏–π –∞–≥–µ–Ω—Ç–∞
            max_searches: –ú–∞–∫—Å–∏–º—É–º –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            max_clarifications: –ú–∞–∫—Å–∏–º—É–º —É—Ç–æ—á–Ω—è—é—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
            toolkit: –°–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Å–æ–≤ tools (–∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞)
            langfuse_enabled: –í–∫–ª—é—á–∏—Ç—å —Ç—Ä–µ–π—Å–∏–Ω–≥ Langfuse
            langfuse_public_key: –ü—É–±–ª–∏—á–Ω—ã–π –∫–ª—é—á Langfuse
            langfuse_secret_key: –°–µ–∫—Ä–µ—Ç–Ω—ã–π –∫–ª—é—á Langfuse
            langfuse_host: URL —Ö–æ—Å—Ç–∞ Langfuse
        """
        self.anthropic_api_key = anthropic_api_key
        self.tavily_api_key = tavily_api_key
        self.langfuse_enabled = langfuse_enabled

        # Tools –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ
        self.toolkit = toolkit or list(TOOL_REGISTRY.values())

        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞
        # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç = –±–∞–∑–æ–≤—ã–π –∏–∑ sgr-agent-core + –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö
        self.agent_config = AgentConfig(
            llm=LLMConfig(
                api_key=anthropic_api_key,
                model=model,
                base_url=api_base,
                temperature=temperature,
                max_tokens=max_tokens,
            ),
            search=SearchConfig(
                tavily_api_key=tavily_api_key,
                max_searches=max_searches,
                max_results=10,
            ),
            execution=ExecutionConfig(
                max_iterations=max_iterations,
                max_clarifications=max_clarifications,
            ),
            prompts=PromptsConfig(
                system_prompt_str=SYSTEM_PROMPT_WITH_VALIDATION,
            ),
        )

        # Setup Langfuse –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
        if langfuse_enabled and langfuse_public_key and langfuse_secret_key:
            setup_langfuse(langfuse_public_key, langfuse_secret_key, langfuse_host)

        # OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π –∫–ª–∏–µ–Ω—Ç
        self.client = create_openai_client(api_key=anthropic_api_key, base_url=api_base)

        logger.info(f"Agent initialized with {len(self.toolkit)} tools")
        logger.info(f"Using custom system prompt with data validation instructions")

    @observe(name="deep_research")
    async def research(
        self,
        messages: list[dict],
        user_id: Optional[str] = None,
        session_id: Optional[str] = None,
    ) -> AsyncIterator[ResearchResult]:
        """
        –ü—Ä–æ–≤–µ—Å—Ç–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ –∑–∞–ø—Ä–æ—Å—É —Å —É—á—ë—Ç–æ–º –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞.

        Args:
            messages: –ò—Å—Ç–æ—Ä–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI [{"role": "user/assistant", "content": "..."}]
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è —Ç—Ä–µ–π—Å–∏–Ω–≥–∞ (–¥–ª—è Langfuse)
            session_id: ID —Å–µ—Å—Å–∏–∏ –¥–ª—è —Ç—Ä–µ–π—Å–∏–Ω–≥–∞ (–¥–ª—è Langfuse)

        Yields:
            ResearchResult —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º –∏ —Ñ–∏–Ω–∞–ª—å–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º
        """
        tools_used: list[str] = []

        # Get last user message for logging
        last_query = messages[-1]["content"] if messages else ""

        # Log Langfuse context info
        if self.langfuse_enabled and LANGFUSE_AVAILABLE:
            logger.debug(f"Langfuse tracing active for query: {last_query[:50]}...")

        try:
            # –°–æ–∑–¥–∞—ë–º –∞–≥–µ–Ω—Ç–∞ —Å –∏—Å—Ç–æ—Ä–∏–µ–π —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
            # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –±–µ—Ä—ë—Ç—Å—è –∏–∑ sgr-agent-core –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–µ—Ä–µ–¥–∞–Ω—ã —á–µ—Ä–µ–∑ PromptsConfig
            agent = TelegramResearchAgent(
                task_messages=messages,  # —Ç–æ–ª—å–∫–æ user/assistant —Å–æ–æ–±—â–µ–Ω–∏—è
                openai_client=self.client,
                agent_config=self.agent_config,
                toolkit=self.toolkit,
            )

            # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
            logger.info(f"Starting research ({len(messages)} messages): {last_query[:200]}...")

            context = agent._context
            step_counter = 0
            last_tool_id = ""

            # –ó–∞–ø—É—Å–∫–∞–µ–º execute() –≤ —Ñ–æ–Ω–µ
            execute_task = asyncio.create_task(agent.execute())

            # –°—Ç—Ä–∏–º–∏–º —Å–æ–±—ã—Ç–∏—è —á–µ—Ä–µ–∑ streaming_generator
            try:
                async for sse_event in agent.streaming_generator.stream():
                    # –ü–∞—Ä—Å–∏–º tool_call –∏–∑ SSE —Å–æ–±—ã—Ç–∏—è
                    tool_info = parse_sse_tool_call(sse_event)

                    if tool_info:
                        tool_id, tool_name, arguments = tool_info

                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º reasoning events (–æ–Ω–∏ –∏–º–µ—é—Ç id —Å "-reasoning")
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ action events (id —Å "-action")
                        if "-action" in tool_id and tool_id != last_tool_id:
                            last_tool_id = tool_id
                            step_counter += 1

                            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–µ—Ç–∞–ª–∏ –∏–∑ arguments
                            tool_detail = ""
                            if "query" in arguments:
                                q = arguments["query"]
                                tool_detail = q[:50] + "..." if len(q) > 50 else q
                            elif "research_goal" in arguments:
                                g = arguments["research_goal"]
                                tool_detail = g[:50] + "..." if len(g) > 50 else g
                            elif "title" in arguments:
                                t = arguments["title"]
                                tool_detail = t[:50] + "..." if len(t) > 50 else t

                            emoji, description = get_tool_info(tool_name)
                            if tool_detail:
                                description = f"{description}: {tool_detail}"

                            progress = ResearchProgress(
                                step=step_counter,
                                tool_name=tool_name,
                                tool_emoji=emoji,
                                description=description,
                                searches_done=context.searches_used,
                                is_final=False,
                            )

                            logger.info(f"[Stream] Progress: step={step_counter}, tool={tool_name}")
                            tools_used.append(tool_name)
                            yield ResearchResult(progress=progress)

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º clarification
                    if context.state == AgentStatesEnum.WAITING_FOR_CLARIFICATION:
                        logger.info("Agent waiting for clarification")
                        # –ù–µ –∂–¥—ë–º execute_task - –∞–≥–µ–Ω—Ç –ø—Ä–∏–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
                        execute_task.cancel()
                        break

            except asyncio.CancelledError:
                logger.info("Streaming cancelled")

            # –ñ–¥—ë–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è execute –µ—Å–ª–∏ –µ—â—ë –Ω–µ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –∏ –Ω–µ clarification
            if context.state != AgentStatesEnum.WAITING_FOR_CLARIFICATION and not execute_task.done():
                try:
                    await execute_task
                except asyncio.CancelledError:
                    pass

            # Debug logging
            logger.info(f"Agent state: {context.state}")
            logger.info(f"Execution result: {context.execution_result[:200] if context.execution_result else 'None'}...")
            if hasattr(context, 'current_step_reasoning'):
                logger.info(f"Current step reasoning: {context.current_step_reasoning}")
            if hasattr(context, 'clarification_received'):
                logger.info(f"Clarification received: {context.clarification_received}")

            # tools_used —É–∂–µ —Å–æ–±—Ä–∞–Ω—ã –≤–æ –≤—Ä–µ–º—è streaming

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –æ—à–∏–±–∫—É
            if context.state == AgentStatesEnum.FAILED:
                error_msg = context.execution_result or "–ê–≥–µ–Ω—Ç –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π"
                logger.error(f"Agent failed: {error_msg}")
                yield ResearchResult(
                    is_done=True,
                    error=error_msg,
                    tools_used=tools_used,
                    iterations=context.iteration if hasattr(context, 'iteration') else 0,
                )
                return

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω—É–∂–Ω–æ –ª–∏ —É—Ç–æ—á–Ω–µ–Ω–∏–µ
            if context.state == AgentStatesEnum.WAITING_FOR_CLARIFICATION:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–æ–ø—Ä–æ—Å—ã –∏–∑ current_step_reasoning.function.questions
                clarification_questions = None
                if hasattr(context, 'current_step_reasoning') and context.current_step_reasoning:
                    reasoning = context.current_step_reasoning
                    if hasattr(reasoning, 'function') and reasoning.function:
                        func = reasoning.function
                        if hasattr(func, 'questions') and func.questions:
                            clarification_questions = "\n".join(func.questions)

                logger.info(f"Clarification questions extracted: {clarification_questions}")

                yield ResearchResult(
                    needs_clarification=True,
                    clarification_question=clarification_questions or "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Ç–æ—á–Ω–∏—Ç–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å.",
                    tools_used=tools_used,
                    iterations=context.iteration if hasattr(context, 'iteration') else 0,
                )
                return

            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result_content = context.execution_result or ""
            iterations = context.iteration if hasattr(context, 'iteration') else 0

            yield ResearchResult(
                content=result_content,
                is_done=True,
                tools_used=tools_used,
                iterations=iterations,
            )

            logger.info(f"Research completed. Tools used: {tools_used}")

            # Flush Langfuse traces
            if self.langfuse_enabled and LANGFUSE_AVAILABLE and langfuse_client:
                try:
                    langfuse_client.flush()
                    logger.debug("Langfuse traces flushed")
                except Exception as flush_err:
                    logger.debug(f"Failed to flush Langfuse: {flush_err}")

        except Exception as e:
            logger.error(f"Research failed: {e}")
            yield ResearchResult(
                is_done=True,
                error=str(e),
                tools_used=tools_used,
            )



def create_agent_from_config(config) -> DeepResearchAgent:
    """
    –°–æ–∑–¥–∞—Ç—å –∞–≥–µ–Ω—Ç–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –±–æ—Ç–∞.

    Args:
        config: Config –æ–±—ä–µ–∫—Ç —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏

    Returns:
        DeepResearchAgent –∏–Ω—Å—Ç–∞–Ω—Å
    """
    return DeepResearchAgent(
        anthropic_api_key=config.llm_api_key,
        tavily_api_key=config.tavily_api_key,
        model=config.llm_model,
        api_base=config.llm_api_base,
        temperature=config.llm_temperature,
        max_tokens=config.llm_max_tokens,
        max_iterations=config.sgr_max_iterations,
        max_searches=config.max_searches,
        max_clarifications=config.sgr_max_clarifications,
        langfuse_enabled=config.langfuse_enabled,
        langfuse_public_key=config.langfuse_public_key,
        langfuse_secret_key=config.langfuse_secret_key,
        langfuse_host=config.langfuse_host,
    )
