"""
SGR Agent - –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π Deep Research –∞–≥–µ–Ω—Ç.

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç sgr-agent-core –Ω–∞–ø—Ä—è–º—É—é –±–µ–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞.
–° –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π Langfuse –¥–ª—è observability.
"""
import asyncio
import json
import logging
from dataclasses import dataclass, field
from typing import AsyncIterator, Type, Optional

from openai import AsyncOpenAI
from sgr_agent_core import (
    AgentConfig,
    LLMConfig,
    SearchConfig,
    ExecutionConfig,
    AgentStatesEnum,
    BaseTool,
)
from sgr_agent_core.agents import SGRAgent
from sgr_agent_core.tools import (
    WebSearchTool,
    ExtractPageContentTool,
    GeneratePlanTool,
    ReasoningTool,
    ClarificationTool,
    AdaptPlanTool,
    CreateReportTool,
    FinalAnswerTool,
    NextStepToolsBuilder,
)

logger = logging.getLogger(__name__)

# Registry: tool name -> tool class
TOOL_REGISTRY: dict[str, Type[BaseTool]] = {
    "WebSearchTool": WebSearchTool,
    "ExtractPageContentTool": ExtractPageContentTool,
    "GeneratePlanTool": GeneratePlanTool,
    "ReasoningTool": ReasoningTool,
    "ClarificationTool": ClarificationTool,
    "AdaptPlanTool": AdaptPlanTool,
    "CreateReportTool": CreateReportTool,
    "FinalAnswerTool": FinalAnswerTool,
}


def resolve_tools(tool_names: list[str]) -> list[Type[BaseTool]]:
    """Resolve tool names to tool classes."""
    tools = []
    for name in tool_names:
        if name in TOOL_REGISTRY:
            tools.append(TOOL_REGISTRY[name])
        else:
            logger.warning(f"Unknown tool: {name}, skipping")
    return tools


def parse_sse_tool_call(sse_data: str) -> tuple[str, str, dict] | None:
    """
    Parse SSE event to extract tool call info.

    Returns (tool_call_id, tool_name, arguments) or None if not a tool call.
    """
    if not sse_data.startswith("data: "):
        return None

    json_str = sse_data[6:].strip()  # Remove "data: " prefix
    if json_str == "[DONE]":
        return None

    try:
        data = json.loads(json_str)
        choices = data.get("choices", [])
        if not choices:
            return None

        delta = choices[0].get("delta", {})
        tool_calls = delta.get("tool_calls")

        if tool_calls and len(tool_calls) > 0:
            tc = tool_calls[0]
            tool_id = tc.get("id", "")
            func = tc.get("function", {})
            tool_name = func.get("name", "")
            arguments_str = func.get("arguments", "{}")

            # Parse arguments JSON
            try:
                arguments = json.loads(arguments_str) if arguments_str else {}
            except json.JSONDecodeError:
                arguments = {}

            if tool_name:
                return (tool_id, tool_name, arguments)

    except json.JSONDecodeError:
        pass

    return None

# Try to import langfuse and openinference
try:
    from langfuse import observe, Langfuse
    LANGFUSE_AVAILABLE = True
    langfuse_client = None  # Will be initialized if enabled
except ImportError:
    LANGFUSE_AVAILABLE = False
    observe = lambda *args, **kwargs: lambda f: f  # no-op decorator
    langfuse_client = None
    Langfuse = None

# Try to import OpenAI instrumentor for detailed LLM tracing
try:
    from openinference.instrumentation.openai import OpenAIInstrumentor
    OPENAI_INSTRUMENTOR_AVAILABLE = True
except ImportError:
    OPENAI_INSTRUMENTOR_AVAILABLE = False
    OpenAIInstrumentor = None


def setup_langfuse(
    public_key: str,
    secret_key: str,
    host: str = "https://cloud.langfuse.com",
) -> bool:
    """
    Setup Langfuse environment variables for tracing.
    Returns True if setup successful.
    """
    global langfuse_client
    try:
        import os
        os.environ["LANGFUSE_PUBLIC_KEY"] = public_key
        os.environ["LANGFUSE_SECRET_KEY"] = secret_key
        os.environ["LANGFUSE_HOST"] = host

        if LANGFUSE_AVAILABLE and Langfuse:
            # Initialize client to ensure connection works
            langfuse_client = Langfuse(
                public_key=public_key,
                secret_key=secret_key,
                host=host,
            )
            logger.info(f"Langfuse configured (host: {host})")

            # Enable OpenAI instrumentation for detailed LLM tracing
            if OPENAI_INSTRUMENTOR_AVAILABLE and OpenAIInstrumentor:
                OpenAIInstrumentor().instrument()
                logger.info("OpenAI instrumentation enabled (OpenInference)")

            return True
        else:
            logger.warning("Langfuse not installed")
            return False
    except Exception as e:
        logger.warning(f"Failed to setup Langfuse: {e}")
        return False


def create_openai_client(api_key: str, base_url: str) -> AsyncOpenAI:
    """Create standard OpenAI client."""
    return AsyncOpenAI(api_key=api_key, base_url=base_url)


@dataclass
class ResearchProgress:
    """–ü—Ä–æ–≥—Ä–µ—Å—Å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é."""

    step: int = 0
    tool_name: str = ""
    tool_emoji: str = ""
    description: str = ""
    searches_done: int = 0
    is_final: bool = False


# –ú–∞–ø–ø–∏–Ω–≥ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —ç–º–æ–¥–∑–∏ –∏ –æ–ø–∏—Å–∞–Ω–∏—è
TOOL_INFO = {
    "generateplantool": ("üìã", "–ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"),
    "websearchtool": ("üîç", "–ü–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ"),
    "extractpagecontenttool": ("üìÑ", "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"),
    "reasoningtool": ("üß†", "–ê–Ω–∞–ª–∏–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"),
    "clarificationtool": ("‚ùì", "–ó–∞–ø—Ä–æ—Å —É—Ç–æ—á–Ω–µ–Ω–∏—è"),
    "adaptplantool": ("üîÑ", "–ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –ø–ª–∞–Ω–∞"),
    "createreporttool": ("üìù", "–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á—ë—Ç–∞"),
    "finalanswertool": ("‚úÖ", "–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞"),
}


def get_tool_info(tool_name: str) -> tuple[str, str]:
    """–ü–æ–ª—É—á–∏—Ç—å —ç–º–æ–¥–∑–∏ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞."""
    key = tool_name.lower().replace("_", "")
    return TOOL_INFO.get(key, ("üîß", tool_name))


@dataclass
class ResearchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è."""

    content: str = ""
    is_done: bool = False
    needs_clarification: bool = False
    clarification_question: str | None = None
    tools_used: list[str] = field(default_factory=list)
    iterations: int = 0
    error: str | None = None
    progress: ResearchProgress | None = None  # –î–æ–±–∞–≤–ª–µ–Ω–æ –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞


class TelegramResearchAgent(SGRAgent):
    """
    –ö–∞—Å—Ç–æ–º–Ω—ã–π SGR –∞–≥–µ–Ω—Ç —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º tools.

    –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç _prepare_tools() –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
    –ø—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ –ª–∏–º–∏—Ç–æ–≤ –∏—Ç–µ—Ä–∞—Ü–∏–π, clarifications –∏ –ø–æ–∏—Å–∫–æ–≤.

    Tools –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ config, –∞ –Ω–µ —Ö–∞—Ä–¥–∫–æ–¥—è—Ç—Å—è.
    """

    def __init__(
        self,
        task_messages: list[dict],
        openai_client: AsyncOpenAI,
        agent_config: AgentConfig,
        toolkit: list[Type[BaseTool]],
        **kwargs,
    ):
        super().__init__(
            task_messages=task_messages,
            openai_client=openai_client,
            agent_config=agent_config,
            toolkit=toolkit,
            **kwargs,
        )

    async def _prepare_tools(self):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ tools –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è.

        –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —É–±–∏—Ä–∞–µ—Ç tools –ø—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ –ª–∏–º–∏—Ç–æ–≤:
        - max_iterations ‚Üí —Ç–æ–ª—å–∫–æ CreateReportTool, FinalAnswerTool
        - max_clarifications ‚Üí —É–±—Ä–∞—Ç—å ClarificationTool
        - max_searches ‚Üí —É–±—Ä–∞—Ç—å WebSearchTool
        """
        tools = set(self.toolkit)

        # –ü—Ä–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–∏ –ª–∏–º–∏—Ç–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π - —Ç–æ–ª—å–∫–æ –∑–∞–≤–µ—Ä—à–∞—é—â–∏–µ tools
        if self._context.iteration >= self.config.execution.max_iterations:
            logger.info(f"Max iterations reached ({self._context.iteration}), limiting to final tools")
            tools = {CreateReportTool, FinalAnswerTool}

        # –ò—Å—á–µ—Ä–ø–∞–ª–∏ clarifications - —É–±—Ä–∞—Ç—å ClarificationTool
        if self._context.clarifications_used >= self.config.execution.max_clarifications:
            logger.debug(f"Max clarifications reached ({self._context.clarifications_used})")
            tools -= {ClarificationTool}

        # –ò—Å—á–µ—Ä–ø–∞–ª–∏ –ø–æ–∏—Å–∫–∏ - —É–±—Ä–∞—Ç—å WebSearchTool
        if self._context.searches_used >= self.config.search.max_searches:
            logger.debug(f"Max searches reached ({self._context.searches_used})")
            tools -= {WebSearchTool}

        return NextStepToolsBuilder.build_NextStepTools(list(tools))


class DeepResearchAgent:
    """
    Deep Research –∞–≥–µ–Ω—Ç –Ω–∞ –±–∞–∑–µ SGR Agent Core.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Schema-Guided Reasoning –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
    —Å –ø–æ–∏—Å–∫–æ–º –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ —á–µ—Ä–µ–∑ Tavily.

    –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (tools, prompts) –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –∏–∑ config.yaml.
    """

    # –î–µ—Ñ–æ–ª—Ç–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω –≤ –∫–æ–Ω—Ñ–∏–≥–µ)
    DEFAULT_SYSTEM_PROMPT = (
        "–¢—ã - –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. "
        "–í–°–ï–ì–î–ê –æ—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —è–∑—ã–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∏–ª–∏ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. "
        "–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç—ã —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –∏ —Å–ø–∏—Å–∫–∞–º–∏ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ —á—Ç–µ–Ω–∏—è."
    )

    def __init__(
        self,
        anthropic_api_key: str,
        tavily_api_key: str,
        model: str = "claude-haiku-4-5",
        api_base: str = "https://api.anthropic.com/v1",
        temperature: float = 0.4,
        max_tokens: int = 8000,
        max_iterations: int = 10,
        max_searches: int = 4,
        max_clarifications: int = 3,
        # Tools and prompts from config
        toolkit: list[Type[BaseTool]] | None = None,
        system_prompt: str | None = None,
        # Langfuse settings
        langfuse_enabled: bool = False,
        langfuse_public_key: str = "",
        langfuse_secret_key: str = "",
        langfuse_host: str = "https://cloud.langfuse.com",
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞.

        Args:
            anthropic_api_key: API –∫–ª—é—á LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
            tavily_api_key: API –∫–ª—é—á Tavily –¥–ª—è –≤–µ–±-–ø–æ–∏—Å–∫–∞
            model: –ú–æ–¥–µ–ª—å LLM
            api_base: Base URL API
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            max_tokens: –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤
            max_iterations: –ú–∞–∫—Å–∏–º—É–º –∏—Ç–µ—Ä–∞—Ü–∏–π –∞–≥–µ–Ω—Ç–∞
            max_searches: –ú–∞–∫—Å–∏–º—É–º –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            max_clarifications: –ú–∞–∫—Å–∏–º—É–º —É—Ç–æ—á–Ω—è—é—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
            toolkit: –°–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Å–æ–≤ tools (–∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞)
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞)
            langfuse_enabled: –í–∫–ª—é—á–∏—Ç—å —Ç—Ä–µ–π—Å–∏–Ω–≥ Langfuse
            langfuse_public_key: –ü—É–±–ª–∏—á–Ω—ã–π –∫–ª—é—á Langfuse
            langfuse_secret_key: –°–µ–∫—Ä–µ—Ç–Ω—ã–π –∫–ª—é—á Langfuse
            langfuse_host: URL —Ö–æ—Å—Ç–∞ Langfuse
        """
        self.anthropic_api_key = anthropic_api_key
        self.tavily_api_key = tavily_api_key
        self.langfuse_enabled = langfuse_enabled

        # Tools –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ
        self.toolkit = toolkit or list(TOOL_REGISTRY.values())

        # System prompt –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π
        self.system_prompt = system_prompt or self.DEFAULT_SYSTEM_PROMPT

        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞
        self.agent_config = AgentConfig(
            llm=LLMConfig(
                api_key=anthropic_api_key,
                model=model,
                base_url=api_base,
                temperature=temperature,
                max_tokens=max_tokens,
            ),
            search=SearchConfig(
                tavily_api_key=tavily_api_key,
                max_searches=max_searches,
                max_results=10,
            ),
            execution=ExecutionConfig(
                max_iterations=max_iterations,
                max_clarifications=max_clarifications,
            ),
        )

        # Setup Langfuse –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
        if langfuse_enabled and langfuse_public_key and langfuse_secret_key:
            setup_langfuse(langfuse_public_key, langfuse_secret_key, langfuse_host)

        # OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π –∫–ª–∏–µ–Ω—Ç
        self.client = create_openai_client(api_key=anthropic_api_key, base_url=api_base)

        logger.info(f"Agent initialized with {len(self.toolkit)} tools")

    @observe(name="deep_research")
    async def research(
        self,
        messages: list[dict],
        user_id: Optional[str] = None,
        session_id: Optional[str] = None,
    ) -> AsyncIterator[ResearchResult]:
        """
        –ü—Ä–æ–≤–µ—Å—Ç–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ –∑–∞–ø—Ä–æ—Å—É —Å —É—á—ë—Ç–æ–º –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞.

        Args:
            messages: –ò—Å—Ç–æ—Ä–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI [{"role": "user/assistant", "content": "..."}]
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è —Ç—Ä–µ–π—Å–∏–Ω–≥–∞ (–¥–ª—è Langfuse)
            session_id: ID —Å–µ—Å—Å–∏–∏ –¥–ª—è —Ç—Ä–µ–π—Å–∏–Ω–≥–∞ (–¥–ª—è Langfuse)

        Yields:
            ResearchResult —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º –∏ —Ñ–∏–Ω–∞–ª—å–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º
        """
        tools_used: list[str] = []

        # Get last user message for logging
        last_query = messages[-1]["content"] if messages else ""

        # Log Langfuse context info
        if self.langfuse_enabled and LANGFUSE_AVAILABLE:
            logger.debug(f"Langfuse tracing active for query: {last_query[:50]}...")

        try:
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –Ω–∞—á–∞–ª–æ
            messages_with_system = [
                {"role": "system", "content": self.system_prompt},
                *messages,
            ]

            # –°–æ–∑–¥–∞—ë–º –∞–≥–µ–Ω—Ç–∞ —Å –ø–æ–ª–Ω–æ–π –∏—Å—Ç–æ—Ä–∏–µ–π —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º TelegramResearchAgent —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º tools
            agent = TelegramResearchAgent(
                task_messages=messages_with_system,
                openai_client=self.client,
                agent_config=self.agent_config,
                toolkit=self.toolkit,
            )

            # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
            logger.info(f"Starting research ({len(messages)} messages): {last_query[:200]}...")

            context = agent._context
            step_counter = 0
            last_tool_id = ""

            # –ó–∞–ø—É—Å–∫–∞–µ–º execute() –≤ —Ñ–æ–Ω–µ
            execute_task = asyncio.create_task(agent.execute())

            # –°—Ç—Ä–∏–º–∏–º —Å–æ–±—ã—Ç–∏—è —á–µ—Ä–µ–∑ streaming_generator
            try:
                async for sse_event in agent.streaming_generator.stream():
                    # –ü–∞—Ä—Å–∏–º tool_call –∏–∑ SSE —Å–æ–±—ã—Ç–∏—è
                    tool_info = parse_sse_tool_call(sse_event)

                    if tool_info:
                        tool_id, tool_name, arguments = tool_info

                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º reasoning events (–æ–Ω–∏ –∏–º–µ—é—Ç id —Å "-reasoning")
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ action events (id —Å "-action")
                        if "-action" in tool_id and tool_id != last_tool_id:
                            last_tool_id = tool_id
                            step_counter += 1

                            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–µ—Ç–∞–ª–∏ –∏–∑ arguments
                            tool_detail = ""
                            if "query" in arguments:
                                q = arguments["query"]
                                tool_detail = q[:50] + "..." if len(q) > 50 else q
                            elif "research_goal" in arguments:
                                g = arguments["research_goal"]
                                tool_detail = g[:50] + "..." if len(g) > 50 else g
                            elif "title" in arguments:
                                t = arguments["title"]
                                tool_detail = t[:50] + "..." if len(t) > 50 else t

                            emoji, description = get_tool_info(tool_name)
                            if tool_detail:
                                description = f"{description}: {tool_detail}"

                            progress = ResearchProgress(
                                step=step_counter,
                                tool_name=tool_name,
                                tool_emoji=emoji,
                                description=description,
                                searches_done=context.searches_used,
                                is_final=False,
                            )

                            logger.info(f"[Stream] Progress: step={step_counter}, tool={tool_name}")
                            tools_used.append(tool_name)
                            yield ResearchResult(progress=progress)

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º clarification
                    if context.state == AgentStatesEnum.WAITING_FOR_CLARIFICATION:
                        logger.info("Agent waiting for clarification")
                        break

            except asyncio.CancelledError:
                logger.info("Streaming cancelled")

            # –ñ–¥—ë–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è execute –µ—Å–ª–∏ –µ—â—ë –Ω–µ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è
            if not execute_task.done():
                try:
                    await execute_task
                except asyncio.CancelledError:
                    pass

            # Debug logging
            logger.info(f"Agent state: {context.state}")
            logger.info(f"Execution result: {context.execution_result[:200] if context.execution_result else 'None'}...")
            if hasattr(context, 'current_step_reasoning'):
                logger.info(f"Current step reasoning: {context.current_step_reasoning}")
            if hasattr(context, 'clarification_received'):
                logger.info(f"Clarification received: {context.clarification_received}")

            # tools_used —É–∂–µ —Å–æ–±—Ä–∞–Ω—ã –≤–æ –≤—Ä–µ–º—è streaming

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –æ—à–∏–±–∫—É
            if context.state == AgentStatesEnum.FAILED:
                error_msg = context.execution_result or "–ê–≥–µ–Ω—Ç –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π"
                logger.error(f"Agent failed: {error_msg}")
                yield ResearchResult(
                    is_done=True,
                    error=error_msg,
                    tools_used=tools_used,
                    iterations=context.iteration if hasattr(context, 'iteration') else 0,
                )
                return

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω—É–∂–Ω–æ –ª–∏ —É—Ç–æ—á–Ω–µ–Ω–∏–µ
            if context.state == AgentStatesEnum.WAITING_FOR_CLARIFICATION:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–æ–ø—Ä–æ—Å—ã –∏–∑ current_step_reasoning.function.questions
                clarification_questions = None
                if hasattr(context, 'current_step_reasoning') and context.current_step_reasoning:
                    reasoning = context.current_step_reasoning
                    if hasattr(reasoning, 'function') and reasoning.function:
                        func = reasoning.function
                        if hasattr(func, 'questions') and func.questions:
                            clarification_questions = "\n".join(func.questions)

                logger.info(f"Clarification questions extracted: {clarification_questions}")

                yield ResearchResult(
                    needs_clarification=True,
                    clarification_question=clarification_questions or "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Ç–æ—á–Ω–∏—Ç–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å.",
                    tools_used=tools_used,
                    iterations=context.iteration if hasattr(context, 'iteration') else 0,
                )
                return

            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result_content = context.execution_result or ""
            iterations = context.iteration if hasattr(context, 'iteration') else 0

            yield ResearchResult(
                content=result_content,
                is_done=True,
                tools_used=tools_used,
                iterations=iterations,
            )

            logger.info(f"Research completed. Tools used: {tools_used}")

            # Flush Langfuse traces
            if self.langfuse_enabled and LANGFUSE_AVAILABLE and langfuse_client:
                try:
                    langfuse_client.flush()
                    logger.debug("Langfuse traces flushed")
                except Exception as flush_err:
                    logger.debug(f"Failed to flush Langfuse: {flush_err}")

        except Exception as e:
            logger.error(f"Research failed: {e}")
            yield ResearchResult(
                is_done=True,
                error=str(e),
                tools_used=tools_used,
            )



def create_agent_from_config(config) -> DeepResearchAgent:
    """
    –°–æ–∑–¥–∞—Ç—å –∞–≥–µ–Ω—Ç–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –±–æ—Ç–∞.

    Args:
        config: Config –æ–±—ä–µ–∫—Ç —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏

    Returns:
        DeepResearchAgent –∏–Ω—Å—Ç–∞–Ω—Å
    """
    return DeepResearchAgent(
        anthropic_api_key=config.llm_api_key,
        tavily_api_key=config.tavily_api_key,
        model=config.llm_model,
        api_base=config.llm_api_base,
        temperature=config.llm_temperature,
        max_tokens=config.llm_max_tokens,
        max_iterations=config.sgr_max_iterations,
        max_searches=config.max_searches,
        max_clarifications=config.sgr_max_clarifications,
        langfuse_enabled=config.langfuse_enabled,
        langfuse_public_key=config.langfuse_public_key,
        langfuse_secret_key=config.langfuse_secret_key,
        langfuse_host=config.langfuse_host,
    )
